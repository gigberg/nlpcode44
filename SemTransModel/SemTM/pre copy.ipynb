{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对citod数据处理\n",
    "1. 删除长度为奇数的对话(实际是在data_load.py中处理的)\n",
    "2. 去除标点, 以防bert的tokenizer分词会自动将标点识别为单个token(实际是在data_load.py中处理的)\n",
    "3. 修改id,使得全局id不发生重复(实际是在gcn_data.py中处理,或者在pre.ipynb中处理的)\n",
    "4. 删除长度为2的对话, 实际上在class dialogue_flow中处理的\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "126\n",
      "246\n",
      "278\n",
      "346\n",
      "405\n",
      "423\n",
      "558\n",
      "582\n",
      "802\n",
      "853\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "id_list = []\n",
    "data_path = [f'./data/{x}/{x}_{y}.json' for x in ['navigate', 'calendar', 'weather'] for y in ['train', 'dev', 'test']]\n",
    "for path in data_path:\n",
    "    with open(path) as f:\n",
    "        raw_data = json.load(f)\n",
    "    for dialogue_components_item in raw_data:\n",
    "        id_list.append(dialogue_components_item['id'])\n",
    "\n",
    "for x in set(id_list):\n",
    "    if id_list.count(x) > 1:\n",
    "        print(x)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "# data_path = [f'./data/{x}/{x}_{y}.json' for x in ['navigate', 'calendar', 'weather'] for y in ['train', 'dev', 'test']] + [f'./data/{x}/entities.json' for x in ['navigate', 'calendar', 'weather']]\n",
    "data_path = [f'./data/{x}/{x}_{y}.json' for x in ['navigate', 'calendar', 'weather'] for y in ['train', 'dev', 'test']] + ['./data/entities.json']\n",
    "dia_id = 0\n",
    "for path in data_path:\n",
    "    with open(path, 'r+') as f:\n",
    "        raw_data = f.read()\n",
    "        # 0. 去除全文中的p_._f_._changs的点, 'p_. _f_._changs' in raw_data\n",
    "        raw_data= re.sub(r'(p_\\._f_\\._changs|p_\\.f_\\._changs)', 'p_f_changs', raw_data)\n",
    "        f.seek(0)\n",
    "        f.truncate(0)\n",
    "        f.write(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def _punctuation_string() -> str:\n",
    "    \"\"\"Get unicode punctuation character.\"\"\"\n",
    "    # We treat all non-letter/number ASCII as punctuation.\n",
    "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
    "    # Punctuation class but we treat them as punctuation anyways, for\n",
    "    # consistency.\n",
    "\n",
    "    all_punctuation = ''\n",
    "\n",
    "    # Iterate over all Unicode characters\n",
    "    for code_point in range(0x110000):\n",
    "        char = chr(code_point)\n",
    "        if (code_point >= 33 and code_point <= 47) or (code_point >= 58 and code_point <= 64) or (code_point >= 91 and code_point <= 96) or (code_point >= 123 and code_point <= 126):\n",
    "            all_punctuation += chr(code_point)\n",
    "            continue\n",
    "        cat = unicodedata.category(char)\n",
    "        if cat.startswith(\"P\"):\n",
    "            all_punctuation += chr(code_point)\n",
    "    return all_punctuation\n",
    "\n",
    "# punctuation_translator = str.maketrans('', '', _punctuation_string().replace('_','')) 不该删除标点, 而是标点用空格代替? 然后去除多余空格\n",
    "punc_num = len(_punctuation_string().replace('_',''))\n",
    "punctuation_translator = str.maketrans(_punctuation_string().replace('_',''), ' ' * punc_num)\n",
    "new_tokenizer = lambda x: x.translate(punctuation_translator).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_path = [f'./data/{x}/{x}_{y}.json' for x in ['navigate', 'calendar', 'weather'] for y in ['train', 'dev', 'test']]\n",
    "dia_id = 0\n",
    "for path in data_path:\n",
    "    with open(path, 'r+') as f:\n",
    "        raw_data = json.load(f)\n",
    "        new_raw_data = [] #还是别在源list上inplace操作,影响迭代次数本身\n",
    "        for index, item in enumerate(raw_data):\n",
    "            #1. 去除奇数长度\n",
    "            if len(item['dialogue']) % 2: # 去除奇数数据,确保nn.mse loss不报错,否则长度为3的数据中q1,a1,q2中,sent_q=2,2,768但entity=2,1,768, shpae不同,mse报错\n",
    "                continue # 不该continue, 而是要del掉, continue导致数据仍然存在,然后源数据上del导致影响for迭代次数\n",
    "                del raw_data\n",
    "            if len(item['dialogue']) == 1: # 去除不足两轮对话数据\n",
    "                continue\n",
    "            #2. 去除标点(换成空格,并去除多余空格)\n",
    "            for i, utt in enumerate(item['dialogue']): # item[\"id\"]没用到, 直接调用.values() # dia_dict[item[\"id\"]].append(utt['utterance']) #item_id有重\n",
    "                item['dialogue'][i]['utterance'] = \" \".join(new_tokenizer(utt['utterance']))\n",
    "            #3. 重建序号\n",
    "            item['id'] = dia_id # 注意这里改变了原始listraw_data的内容\n",
    "            dia_id += 1\n",
    "            new_raw_data.append(item)\n",
    "        f.seek(0)\n",
    "        f.truncate(0)\n",
    "        json.dump(new_raw_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 验证\n",
    "import json\n",
    "id_list = []\n",
    "data_path = [f'./data/{x}/{x}_{y}.json' for x in ['navigate', 'calendar', 'weather'] for y in ['train', 'dev', 'test']]\n",
    "for path in data_path:\n",
    "    with open(path) as f:\n",
    "        raw_data = json.load(f)\n",
    "    for dialogue_components_item in raw_data:\n",
    "        id_list.append(dialogue_components_item['id'])\n",
    "\n",
    "for x in set(id_list):\n",
    "    if id_list.count(x) > 1:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/zhoujiaming/kyoto/nlpcode/SemTransModel/SemTM/data\n",
      "/usr/local/zhoujiaming/kyoto/nlpcode/SemTransModel/SemTM\n"
     ]
    }
   ],
   "source": [
    "%cd ./data\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "# Directories containing the files\n",
    "navigate_dir = \"navigate\"\n",
    "weather_dir = \"weather\"\n",
    "calendar_dir = \"calendar\"\n",
    "\n",
    "# Categories to process\n",
    "categories = [\"train.json\", \"dev.json\", \"test.json\"]\n",
    "\n",
    "# Output directory for the concatenated files\n",
    "output_dir = \"citod\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for category in categories:\n",
    "    navigate_file = os.path.join(navigate_dir, f\"navigate_{category}\")\n",
    "    weather_file = os.path.join(weather_dir, f\"weather_{category}\")\n",
    "    calendar_file = os.path.join(calendar_dir, f\"calendar_{category}\")\n",
    "\n",
    "    with open(navigate_file, 'r') as navigate_f, open(weather_file, 'r') as weather_f, open(calendar_file, 'r') as calendar_f:\n",
    "        navigate_data = json.load(navigate_f)\n",
    "        weather_data = json.load(weather_f)\n",
    "        calendar_data = json.load(calendar_f)\n",
    "\n",
    "        concatenated_data=[]\n",
    "        concatenated_data.extend(navigate_data)\n",
    "        concatenated_data.extend(weather_data)\n",
    "        concatenated_data.extend(calendar_data)\n",
    "\n",
    "        output_file = os.path.join(output_dir, f\"{output_dir}_{category}\")\n",
    "        with open(output_file, 'w') as output_f:\n",
    "            json.dump(concatenated_data, output_f, indent=4)\n",
    "\n",
    "for x in [navigate_dir, weather_dir, calendar_dir, output_dir]:\n",
    "    shutil.copy(\"entities.json\"), os.path.join(output_dir, \"entities.json\"))\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        \n",
      "        self.dataset = configs[\"dataset\"]\n",
      "        self.dia_enc = configs[\"dia_enc\"]\n",
      "        self.sent_enc = configs[\"sent_enc\"]\n",
      "        self.flow_type = configs[\"flow_type\"]\n",
      "        self.classifier = configs[\"classifier\"]\n",
      "        \n",
      "        self.save_path = configs[\"save_path\"]\n",
      "        self.device = configs[\"device\"]\n",
      "        \n",
      "        self.hidden_dim = configs[\"hidden_dim\"]\n",
      "        self.dist_emb_size = configs[\"dist_emb_size\"]\n",
      "        self.type_emb_size = configs[\"type_emb_size\"]\n",
      "        self.lstm_hid_size = configs[\"lstm_hid_size\"]\n",
      "        self.conv_hid_size = configs[\"conv_hid_size\"]\n",
      "        self.bert_hid_size = configs[\"bert_hid_size\"]\n",
      "        self.ffnn_hid_size = configs[\"ffnn_hid_size\"]\n",
      "        self.biaffine_size = configs[\"biaffine_size\"]\n",
      "        \n",
      "        self.dilation = configs[\"dilation\"]\n",
      "        \n",
      "        self.emb_dropout = configs[\"emb_dropout\"]\n",
      "        self.conv_dropout = configs[\"conv_dropout\"]\n",
      "        self.out_dropout = configs[\"out_dropout\"]\n",
      "        \n",
      "        # 训练相关\n",
      "        self.epochs = configs[\"epochs\"]\n",
      "        self.batch_size = configs[\"batch_size\"]\n",
      "        self.seed = configs[\"seed\"]\n",
      "        self.device = configs[\"device\"]\n",
      "        self.evaluate = configs[\"evaluate\"]\n",
      "        self.retoken = configs[\"retoken\"]\n",
      "        \n",
      "        # 优化器权重衰减\n",
      "        self.clip_grad_norm = configs[\"clip_grad_norm\"]\n",
      "        self.learning_rate = configs[\"learning_rate\"]\n",
      "        self.weight_decay = configs[\"weight_decay\"]\n",
      "        self.bert_name = configs[\"bert_name\"]\n",
      "        self.bert_learning_rate = configs[\"bert_learning_rate\"]\n",
      "        self.warm_factor = configs[\"warm_factor\"]\n",
      "        \n",
      "        # other\n",
      "        self.use_bert_last_4_layers = configs[\"use_bert_last_4_layers\"]\n",
      "        self.noise_rate = configs[\"noise_rate\"]\n",
      "        self.flow_bias = configs[\"flow_bias\"]\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "x = \"\"\"\n",
    "    parser.add_argument('--dataset', type=str, default=\"navigate\")\n",
    "    parser.add_argument('--dia_enc', type=str, default=\"RNN_ParaGate_1\")\n",
    "    parser.add_argument('--sent_enc', type=str, default=\"UniAttention_1\")\n",
    "    parser.add_argument('--flow_type', type=str, default=\"FlowCellQA_E_2\")\n",
    "    parser.add_argument('--classifier', type=str, default=\"BiaffineClassifier\")\n",
    "\n",
    "    parser.add_argument('--save_path', type=str, default='./model.pt')\n",
    "    parser.add_argument('--device', type=int, default=0)\n",
    "\n",
    "    parser.add_argument('--hidden_dim', type=int, default=768)\n",
    "    parser.add_argument('--dist_emb_size', type=int)\n",
    "    parser.add_argument('--type_emb_size', type=int)\n",
    "    parser.add_argument('--lstm_hid_size', type=int)\n",
    "    parser.add_argument('--conv_hid_size', type=int)\n",
    "    parser.add_argument('--bert_hid_size', type=int)\n",
    "    parser.add_argument('--ffnn_hid_size', type=int)\n",
    "    parser.add_argument('--biaffine_size', type=int)\n",
    "\n",
    "    parser.add_argument('--dilation', type=str, help=\"e.g. 1,2,3\") # default=[1,2,3]不能传list\n",
    "\n",
    "    parser.add_argument('--emb_dropout', type=float)\n",
    "    parser.add_argument('--conv_dropout', type=float)\n",
    "    parser.add_argument('--out_dropout', type=float)\n",
    "\n",
    "    # 训练相关\n",
    "    parser.add_argument('--epochs', type=int, default=20)\n",
    "    parser.add_argument('--batch_size', type=int, default=8)\n",
    "    parser.add_argument('--seed', type=int, default=123)\n",
    "    parser.add_argument('--device', type=int, default=1)\n",
    "    parser.add_argument('--evaluate', action='store_true')\n",
    "    parser.add_argument('--retoken', action='store_true')\n",
    "\n",
    "    # 优化器权重衰减\n",
    "    parser.add_argument('--clip_grad_norm', type=float, default=1.0)\n",
    "    parser.add_argument('--learning_rate', type=float, default=5e-5)\n",
    "    parser.add_argument('--weight_decay', type=float, default=1e-5)\n",
    "    parser.add_argument('--bert_name', type=str)\n",
    "    parser.add_argument('--bert_learning_rate', type=float, default=1e-5)\n",
    "    parser.add_argument('--warm_factor', type=float)\n",
    "\n",
    "    # other\n",
    "    parser.add_argument('--use_bert_last_4_layers', type=int, help=\"1: true, 0: false\")\n",
    "    parser.add_argument('--noise_rate', type=float, default=0.00)\n",
    "    parser.add_argument('--flow_bias', action='store_true')\n",
    "\"\"\"\n",
    "import re\n",
    "for line in x.split('\\n'):\n",
    "    if match := re.search('--(.*?)\\'', line):\n",
    "        arg = match.group(1)\n",
    "        print(f'{\" \"*8}self.{arg} = configs[\"{arg}\"]')\n",
    "    else: print(' '*8 + line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \n",
      "  \"dataset\": \"navigate\",\n",
      "  \"dia_enc\": \"RNN_ParaGate_1\",\n",
      "  \"sent_enc\": \"UniAttention_1\",\n",
      "  \"flow_type\": \"FlowCellQA_E_2\",\n",
      "  \"classifier\": \"BiaffineClassifier\",\n",
      "  \n",
      "  \"save_path\": './model.pt',\n",
      "  \"device\": 0,\n",
      "  \n",
      "  \"hidden_dim\": 768,\n",
      "  \"dist_emb_size\": 0,\n",
      "  \"type_emb_size\": 0,\n",
      "  \"lstm_hid_size\": 0,\n",
      "  \"conv_hid_size\": 0,\n",
      "  \"bert_hid_size\": 0,\n",
      "  \"ffnn_hid_size\": 0,\n",
      "  \"biaffine_size\": 0,\n",
      "  \n",
      "  \"dilation\": 0,\n",
      "  \n",
      "  \"emb_dropout\": 0,\n",
      "  \"conv_dropout\": 0,\n",
      "  \"out_dropout\": 0,\n",
      "  \n",
      "  # 训练相关\n",
      "  \"epochs\": 20,\n",
      "  \"batch_size\": 8,\n",
      "  \"seed\": 123,\n",
      "  \"device\": 1,\n",
      "  \"evaluate\": 0,\n",
      "  \"retoken\": 0,\n",
      "  \n",
      "  # 优化器权重衰减\n",
      "  \"clip_grad_norm\": 1.0,\n",
      "  \"learning_rate\": 5e-5,\n",
      "  \"weight_decay\": 1e-5,\n",
      "  \"bert_name\": 0,\n",
      "  \"bert_learning_rate\": 1e-5,\n",
      "  \"warm_factor\": 0,\n",
      "  \n",
      "  # other\n",
      "  \"use_bert_last_4_layers\": 0,\n",
      "  \"noise_rate\": 0.00,\n",
      "  \"flow_bias\": 0,\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "print('{')\n",
    "for line in x.split('\\n'):\n",
    "    if match := re.search('--(.*?)\\'', line):\n",
    "        arg = match.group(1)\n",
    "        if match := re.search(r'--(.*?)\\'.*default=(.*)\\)', line):\n",
    "            val = match.group(2)\n",
    "            print(f'{\" \"*2}\"{arg}\": {val},')\n",
    "            continue\n",
    "        print(f'{\" \"*2}\"{arg}\": 0,')\n",
    "    else: print(' '*2 + line.strip())\n",
    "print('}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \"\"\"{\n",
    "  \"dataset\": \"navigate\",\n",
    "  \"bert_name\": \"bert_base_cased\",\n",
    "  \"dia_enc\": \"MultiUniAtten\",\n",
    "  \"sent_enc\": \"MultiUniAtten\",\n",
    "  \"flow_type\": \"FlowCellQA\",\n",
    "  \"classifier\": \"BiaffineClassifier\",\n",
    "\n",
    "  \"save_path\": './model.pt',\n",
    "\n",
    "  # 模型形状\n",
    "  \"hidden_size\": 768,\n",
    "  \"max_position_embeddings\": 2000,\n",
    "  \"type_emb_size\": 2,\n",
    "  \"ffnn_hid_size\": 3072,\n",
    "  \"head_count\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "\n",
    "  \"dilation\": [1,2,3],\n",
    "\n",
    "  \"attn_dropout\": 0.1,\n",
    "  \"emb_dropout\": 0.5,\n",
    "  \"conv_dropout\": 0.5,\n",
    "  # \"biaffine_dropout\": 0.33,\n",
    "\n",
    "  # 训练相关\n",
    "  \"epochs\": 20,\n",
    "  \"batch_size\": 8,\n",
    "  \"seed\": 123,\n",
    "  \"device\": 1,\n",
    "  \"evaluate\": False,\n",
    "  \"retoken\": False,\n",
    "\n",
    "  # 优化器权重衰减\n",
    "  \"learning_rate\": 1e-3,\n",
    "  \"weight_decay\": 0,\n",
    "  \"clip_grad_norm\": 1.0,\n",
    "\n",
    "  \"bert_learning_rate\": 1e-5,\n",
    "  \"warm_factor\": 0.1,\n",
    "  \"layer_norm_eps\": 1e-12\n",
    "\n",
    "  # other\n",
    "  \"noise_rate\": 0.0,\n",
    "  \"flow_bias\": 0,\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        {\n",
      "        self.dataset = config[\"dataset\"]\n",
      "        self.bert_name = config[\"bert_name\"]\n",
      "        self.dia_enc = config[\"dia_enc\"]\n",
      "        self.sent_enc = config[\"sent_enc\"]\n",
      "        self.flow_type = config[\"flow_type\"]\n",
      "        self.classifier = config[\"classifier\"]\n",
      "        \n",
      "        self.save_path = config[\"save_path\"]\n",
      "        \n",
      "        # 模型形状\n",
      "        self.hidden_size = config[\"hidden_size\"]\n",
      "        self.max_position_embeddings = config[\"max_position_embeddings\"]\n",
      "        self.type_emb_size = config[\"type_emb_size\"]\n",
      "        self.ffnn_hid_size = config[\"ffnn_hid_size\"]\n",
      "        self.head_count = config[\"head_count\"]\n",
      "        self.num_hidden_layers = config[\"num_hidden_layers\"]\n",
      "        \n",
      "        self.dilation = config[\"dilation\"]\n",
      "        \n",
      "        self.attn_dropout = config[\"attn_dropout\"]\n",
      "        self.emb_dropout = config[\"emb_dropout\"]\n",
      "        self.conv_dropout = config[\"conv_dropout\"]\n",
      "        self.biaffine_dropout = config[\"biaffine_dropout\"]\n",
      "        \n",
      "        # 训练相关\n",
      "        self.epochs = config[\"epochs\"]\n",
      "        self.batch_size = config[\"batch_size\"]\n",
      "        self.seed = config[\"seed\"]\n",
      "        self.device = config[\"device\"]\n",
      "        self.evaluate = config[\"evaluate\"]\n",
      "        self.retoken = config[\"retoken\"]\n",
      "        \n",
      "        # 优化器权重衰减\n",
      "        self.learning_rate = config[\"learning_rate\"]\n",
      "        self.weight_decay = config[\"weight_decay\"]\n",
      "        self.clip_grad_norm = config[\"clip_grad_norm\"]\n",
      "        \n",
      "        self.bert_learning_rate = config[\"bert_learning_rate\"]\n",
      "        self.warm_factor = config[\"warm_factor\"]\n",
      "        self.layer_norm_eps = config[\"layer_norm_eps\"]\n",
      "        \n",
      "        # other\n",
      "        self.noise_rate = config[\"noise_rate\"]\n",
      "        self.flow_bias = config[\"flow_bias\"]\n",
      "        }\n"
     ]
    }
   ],
   "source": [
    "# \"noise_rate\": 0.0 -> self.emb_dropout = config[\"emb_dropout\"]\n",
    "import re\n",
    "for line in y.split('\\n'):\n",
    "    if match := re.search(r'\\\"(.*?)\\\"', line):\n",
    "        arg = match.group(1)\n",
    "        print(f'{\" \"*8}self.{arg} = config[\"{arg}\"]')\n",
    "\n",
    "    else: print(' '*8 + line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = \"\"\"{\n",
    "  \"dataset\": \"navigate\",\n",
    "  \"bert_name\": \"bert_base_cased\",\n",
    "  \"dia_enc\": \"MultiUniAtten\",\n",
    "  \"sent_enc\": \"SentUniAtten\",\n",
    "  \"flow_type\": \"FlowCellQA\",\n",
    "  \"classifier\": \"BiaffineClassifier\",\n",
    "\n",
    "  \"save_path\": \"./model.pt\",\n",
    "\n",
    "  \"hidden_size\": 768,\n",
    "  \"max_position_embeddings\": 2000,\n",
    "  \"type_emb_size\": 2,\n",
    "  \"cln_hid_size\": 3072,\n",
    "  \"ffnn_hid_size\": 3072,\n",
    "  \"head_count\": 12,\n",
    "  \"num_hidden_layers\": 12,\n",
    "  \"flow_size\": 2,\n",
    "\n",
    "  \"dilation\": [1,2,3],\n",
    "\n",
    "  \"attn_dropout\": 0.1,\n",
    "  \"emb_dropout\": 0.5,\n",
    "  \"conv_dropout\": 0.5,\n",
    "  \"layer_norm_eps\": 1e-6,\n",
    "\n",
    "  \"epochs\": 20,\n",
    "  \"batch_size\": 8,\n",
    "  \"seed\": 123,\n",
    "  \"device\": 0,\n",
    "  \"evaluate\": False,\n",
    "  \"retoken\": False,\n",
    "\n",
    "  \"bert_learning_rate\": 1e-5,\n",
    "  \"learning_rate\": 5e-5,\n",
    "  \"warm_factor\": 0.1,\n",
    "\n",
    "  \"clip_grad_norm\": 1.0,\n",
    "  \"weight_decay\": 0,\n",
    "\n",
    "  \"noise_rate\": 0.0,\n",
    "  \"flow_bias\": 0\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epochs': 20,\n",
       " 'batch_size': 8,\n",
       " 'seed': 123,\n",
       " 'device': 0,\n",
       " 'evaluate': False,\n",
       " 'retoken': False}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "json.loads(dd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd  = \"\"\"{\n",
    "  \"epochs\": 20,\n",
    "  \"batch_size\": 8,\n",
    "  \"seed\": 123,\n",
    "  \"device\": 0,\n",
    "  \"evaluate\": false,\n",
    "  \"retoken\": false\n",
    "\n",
    "}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2067904596.py, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 25\u001b[0;36m\u001b[0m\n\u001b[0;31m    model = Model().cuda()\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import geotorch\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # One line suffices: Instantiate a linear layer with orthonormal columns\n",
    "        self.linear = nn.Linear(64, 128)\n",
    "        geotorch.orthogonal(self.linear, \"weight\")\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # self.linear is orthogonal and every 3x3 kernel in self.cnn is of rank 1\n",
    "\n",
    "# Use the model as you would normally do. Everything just works\n",
    "model = Model().cuda()\n",
    "\n",
    "# Use your optimizer of choice. Any optimizer works out of the box with any parametrization\n",
    "optim = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import geotorch\n",
    "layer = nn.Linear(2, 2)\n",
    "geotorch.orthogonal(layer, \"weight\")\n",
    "torch.allclose(layer.weight.T @ layer.weight, torch.eye(2,2))\n",
    "\n",
    "\n",
    "# layer = nn.Conv2d(20, 40, 3, 3)  # Make the kernels orthogonal\n",
    "# geotorch.orthogonal(layer, \"weight\")\n",
    "# torch.norm(layer.weight.transpose(-2, -1) @ layer.weight - torch.eye(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FixedRank' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m layer \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m20\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m M \u001b[38;5;241m=\u001b[39m \u001b[43mFixedRank\u001b[49m(layer\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39msize(), rank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m      3\u001b[0m geotorch\u001b[38;5;241m.\u001b[39mregister_parametrization(layer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight\u001b[39m\u001b[38;5;124m\"\u001b[39m, M)\n\u001b[1;32m      4\u001b[0m layer\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m=\u001b[39m M\u001b[38;5;241m.\u001b[39msample()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FixedRank' is not defined"
     ]
    }
   ],
   "source": [
    "layer = nn.Linear(20, 20)\n",
    "M = FixedRank(layer.weight.size(), rank=6)\n",
    "geotorch.register_parametrization(layer, \"weight\", M)\n",
    "layer.weight = M.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1120,  0.7717,  0.8428],\n",
       "        [ 0.0864, -0.1902, -0.4740],\n",
       "        [ 0.1904, -0.8738, -0.6036]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geotorch.GLp(torch.zeros(3,3).size()).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SGDCI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
