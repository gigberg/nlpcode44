{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "[Learn the Basics](intro.html) ||\n",
        "**Quickstart** ||\n",
        "[Tensors](tensorqs_tutorial.html) ||\n",
        "[Datasets & DataLoaders](data_tutorial.html) ||\n",
        "[Transforms](transforms_tutorial.html) ||\n",
        "[Build Model](buildmodel_tutorial.html) ||\n",
        "[Autograd](autogradqs_tutorial.html) ||\n",
        "[Optimization](optimization_tutorial.html) ||\n",
        "[Save & Load Model](saveloadrun_tutorial.html)\n",
        "\n",
        "# Quickstart\n",
        "This section runs through the API for common tasks in machine learning. Refer to the links in each section to dive deeper.\n",
        "\n",
        "## Working with data\n",
        "PyTorch has two [primitives to work with data](https://pytorch.org/docs/stable/data.html):\n",
        "``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset``.\n",
        "``Dataset`` stores the samples and their corresponding labels, and ``DataLoader`` wraps an iterable around\n",
        "the ``Dataset``.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "collapsed": false,
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# ML workflow: preparing data -> creating model -> optimizing parameters -> saving model\n",
        "# Working with data\n",
        "# PyTorch has two primitives to work with data: torch.utils.data.Dataset stores the samples and their corresponding labels, and \n",
        "# torch.utils.data.DataLoader{(N, C, H, W), (N, y)} wraps an iterable batch around the Dataset{N, (C, X_H, X_W, y)}.\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Every TorchVision Dataset includes two arguments: transform and target_transform to modify the samples and labels respectively.\n",
        "from torchvision import datasets # torch.utils.data.Dataset objects\n",
        "from torchvision.transforms import ToTensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PyTorch offers domain-specific libraries such as [TorchText](https://pytorch.org/text/stable/index.html),\n",
        "[TorchVision](https://pytorch.org/vision/stable/index.html), and [TorchAudio](https://pytorch.org/audio/stable/index.html),\n",
        "all of which include datasets. For this tutorial, we  will be using a TorchVision dataset.\n",
        "\n",
        "The ``torchvision.datasets`` module contains ``Dataset`` objects for many real-world vision data like\n",
        "CIFAR, COCO ([full list here](https://pytorch.org/vision/stable/datasets.html)). In this tutorial, we\n",
        "use the FashionMNIST dataset. Every TorchVision ``Dataset`` includes two arguments: ``transform`` and\n",
        "``target_transform`` to modify the samples and labels respectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "collapsed": false,
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Download training data from open datastets\n",
        "# type(training_data).__mro__ = (torchvision.datasets.mnist.FashionMNIST, torchvision.datasets.mnist.MNIST,\n",
        "#                                 torchvision.datasets.vision.VisionDataset, torch.utils.data.dataset.Dataset,\n",
        "#                                   typing.Generic, object)\n",
        "# len(training_data) = 60000, type(training_data[0]) = tuple\n",
        "# training_data[0] = (tensor([[28*28]]),9) -> tuple, the first element of the tuple is tensor because of transform=ToTensor()\n",
        "# training_data[0][0].shape = torch.Size([1, 28, 28]), consisted of float between 0~1\n",
        "training_data = datasets.FashionMNIST( \n",
        "    root=\"../nlpcode/nlpdatahub\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "#Download test data from open datasets.\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"../nlpcode/nlpdatahub\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We pass the ``Dataset`` as an argument to ``DataLoader``. This wraps an iterable over our dataset, and supports\n",
        "automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element\n",
        "in the dataloader iterable will return a batch of 64 features and labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "collapsed": false,
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders X, y iterator.\n",
        "# type(train_dataloader).__mro__ = ((torch.utils.data.dataloader.DataLoader, typing.Generic, object))\n",
        "# len(train_dataloader.dataset) = 60000, len(train_dataloader) = 60000/64 = 938 b atches, \n",
        "# iterator to list: list(train_dataloader) = [[tensor_X, tensor_y],...,num=938]\n",
        "# Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28]), Shape of y: torch.Size([64]) torch.int64\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break # Âè™Âæ™ÁéØ‰∏ÄÊ¨°"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read more about [loading data in PyTorch](data_tutorial.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating Models\n",
        "To define a neural network in PyTorch, we create a class that inherits\n",
        "from [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). We define the layers of the network\n",
        "in the ``__init__`` function and specify how data will pass through the network in the ``forward`` function. To accelerate\n",
        "operations in the neural network, we move it to the GPU if available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "collapsed": false,
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda:0 device\n",
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Creating Models\n",
        "# Get cpu or gpu device for training.\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# To define a neural network in PyTorch, we create a class that inherits from nn.Module. \n",
        "# Define Model\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self): # define the layers of the network \n",
        "        super(NeuralNetwork, self).__init__()  \n",
        "        self.flatten = nn.Flatten() # NCHW([64, 1, 28, 28]) -> flatten(1, -1) = (64 , 28*28)\n",
        "        # ÂâçÈ¶àÁ•ûÁªèÁΩëÁªúÊ®°ÂûãÔºöy = f(ùíòTùíô + ùëè)Ôºåf()‰∏∫Á•ûÁªèÂÖÉÁöÑÈùûÁ∫øÊÄßÊøÄÊ¥ªÂáΩÊï∞ÔºåwT‰∏∫Á•ûÁªèÂÖÉÊùÉÈáç\n",
        "        # ÈùûÁ∫øÊÄßÊøÄÊ¥ªÂáΩÊï∞ReLU=max(0, x)Áî®‰∫é‰∏≠Èó¥Â±ÇÔºåÈùûÁ∫øÊÄßÊøÄÊ¥ªÂáΩÊï∞Sigmoid->(0,1)Áî®‰∫éÊúÄÂêéÁöÑÊ¶ÇÁéáÂÄºËæìÂá∫Â±Ç\n",
        "        # ÊØè‰∏™/Â±ÇÁ•ûÁªèÂÖÉÁî±ÊùÉÈáçlinear_wTÂíåÊøÄÊ¥ªÂáΩÊï∞ReLU(ÊàñSigmoid)ÊûÑÊàêÔºåÊú¨‰æã‰∏≠‰∏∫‰∏âÂ±ÇÁ•ûÁªèÁΩëÁªú(‰∏§‰∏™ReLU+‰∏Ä‰∏™Êºè‰∫ÜÁöÑÁöÑÁî®‰∫éËæìÂá∫ÁöÑSigmoid)\n",
        "        # ËæìÂÖ•Â±Çflatten -> 3‰∏™ÈöêËóèÂ±ÇLinear -> ËæìÂá∫Â±Çsigmoid\n",
        "        self.linear_relu_stack = nn.Sequential(  # input: 64 batches X_(28*28)\n",
        "            nn.Linear(28*28, 512),               # 64 batches {X_(28*28) . wT_(28*28,512) + b_(512)-> y_(512)}\n",
        "            nn.ReLU(),                           # 64 batches {y_(512)-> y_(512)}\n",
        "            nn.Linear(512, 512),                 # 64 batches {X_(512) . wT_(512,512) + b_(512)-> y_(512)}\n",
        "            nn.ReLU(),                           # 64 batches {y_(512)-> y_(512)}\n",
        "            nn.Linear(512, 10),                  # 64 batches {X_(512) . wT_(512,10) + b_(10)-> y_(10)}\n",
        "            # nn.Sigmoid()                       # output: 64 batches {y_(10)}  \n",
        "            # nn.Softmax(dim=1)     \n",
        "        )\n",
        "        # Êú¨‰æã‰∏≠ÂøòËÆ∞Âä†ÂÖ•ÁöÑSigmoidÂáΩÊï∞(sigmoidÂèØ‰ª•ÊîæÂú®ÊîæÂú®Ê®°Âûã‰∏≠ÊàñËÄÖforward‰∏≠)\n",
        "        # 64 batches Sigmoid_{y_(10)-> y_(10)}  \n",
        "        \n",
        "        # Êü•ÁúãÂêÑÂ±ÇÁ•ûÁªèÁΩëÁªú‰∏≠ÁöÑÂèÇÊï∞ÔºöÊùÉÈáçÂíåÂÅèÂ∑Æ\n",
        "        # for name, param in model.named_parameters():\n",
        "        #     print(name, param.shape)\n",
        "        # \n",
        "        # linear_relu_stack.0.weight torch.Size([512, 784]) -> Linear(28*28, 512), LinearÂÄº‰πüÂç≥ÊùÉÈáçÁ≥ªÊï∞Áü©ÈòµËΩ¨ÁΩÆÂêéÁöÑwTÂÄº\n",
        "        # linear_relu_stack.0.bias torch.Size([512])        -> Linear.bias(512)\n",
        "        # linear_relu_stack.2.weight torch.Size([512, 512]) -> Linear(512, 512),\n",
        "        # linear_relu_stack.2.bias torch.Size([512])        -> Linear.bias(512)\n",
        "        # linear_relu_stack.4.weight torch.Size([10, 512])  -> Linear(512, 10) \n",
        "        # linear_relu_stack.4.bias torch.Size([10])         -> Linear.bias(10)\n",
        "        #                   4.bias = list(model.parameters())[5] = tensor([ 0.0398, -0.0169, -0.0100,  0.0044,  \n",
        "        #                        -0.0198, 0.0174, -0.0120,  0.0384, 0.0268, -0.0440], device='cuda:0', requires_grad=True)\n",
        "        # above model.parameters() value is also what in model.pth and model.state_dict()\n",
        "        \n",
        "    def forward(self, x): # specify how data will pass through the network, x.shape = torch.Size([64, 1, 28, 28]) one batch\n",
        "        x = self.flatten(x) # x.shape = torch.Size([64, 784])\n",
        "        logits = self.linear_relu_stack(x) # logit.shape = torch.Size([64, 10])\n",
        "        # logits[0] = tensor([ 0.1264, -0.0450,  0.0593,  0.0600, -0.0301,  0.0103,  0.0152,  0.1127,\n",
        "        #                              0.0805, -0.1390], device='cuda:0', grad_fn=<SelectBackward0>)\n",
        "        # logitsÊ¶ÇÁéáÂÄº‰∏≠ÊúâË¥üÊï∞ÊòØÂõ†‰∏∫ÊúÄÂêé‰∏ÄÂ±ÇÊ≤°Êúâ‰ΩøÁî®SigmiodÊøÄÊ¥ªÂáΩÊï∞ÈôêÂà∂ÂÄºÁöÑËåÉÂõ¥Âà∞(0,1),\n",
        "        # ÊàñËÄÖËØ¥Ê≤°ÊúâsoftmaxÂáΩÊï∞ÈôêÂà∂(0,1)‰∏îÂíå‰∏∫1(Êõ¥ÂÉèÊ¶ÇÁéáÂÄº)\n",
        "        # logits = torch.sigmoid(logits)\n",
        "        return logits\n",
        "\n",
        "# move to GPU, To accelerate operations in the neural network\n",
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "#Âú®tensorboard‰∏≠Êü•ÁúãÊ®°Âûã\n",
        "writer = SummaryWriter('./data/tensorboard')\n",
        "writer.add_graph(model, input_to_model = next(iter(train_dataloader))[0].to(device))\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "graph(%len.1 : int):\n",
            "  %21 : int = prim::Constant[value=1]()\n",
            "  %13 : bool = prim::Constant[value=1]() # /tmp/ipykernel_2042247/2032849203.py:7:2\n",
            "  %5 : NoneType = prim::Constant()\n",
            "  %1 : int = prim::Constant[value=3]() # /tmp/ipykernel_2042247/2032849203.py:6:19\n",
            "  %2 : int = prim::Constant[value=4]() # /tmp/ipykernel_2042247/2032849203.py:6:22\n",
            "  %16 : int = prim::Constant[value=10]() # /tmp/ipykernel_2042247/2032849203.py:8:11\n",
            "  %20 : float = prim::Constant[value=1.]() # /tmp/ipykernel_2042247/2032849203.py:9:18\n",
            "  %4 : int[] = prim::ListConstruct(%1, %2)\n",
            "  %rv.1 : Tensor = aten::zeros(%4, %5, %5, %5, %5) # /tmp/ipykernel_2042247/2032849203.py:6:7\n",
            "  %rv : Tensor = prim::Loop(%len.1, %13, %rv.1) # /tmp/ipykernel_2042247/2032849203.py:7:2\n",
            "    block0(%i.1 : int, %rv.29 : Tensor):\n",
            "      %17 : bool = aten::lt(%i.1, %16) # /tmp/ipykernel_2042247/2032849203.py:8:7\n",
            "      %rv.27 : Tensor = prim::If(%17) # /tmp/ipykernel_2042247/2032849203.py:8:4\n",
            "        block0():\n",
            "          %rv.5 : Tensor = aten::sub(%rv.29, %20, %21) # /tmp/ipykernel_2042247/2032849203.py:9:13\n",
            "          -> (%rv.5)\n",
            "        block1():\n",
            "          %rv.11 : Tensor = aten::add(%rv.29, %20, %21) # /tmp/ipykernel_2042247/2032849203.py:11:13\n",
            "          -> (%rv.11)\n",
            "      -> (%13, %rv.27)\n",
            "  return (%rv)\n",
            "\n",
            "def foo(len: int) -> Tensor:\n",
            "  rv = torch.zeros([3, 4])\n",
            "  rv0 = rv\n",
            "  for i in range(len):\n",
            "    if torch.lt(i, 10):\n",
            "      rv1 = torch.sub(rv0, 1.)\n",
            "    else:\n",
            "      rv1 = torch.add(rv0, 1.)\n",
            "    rv0 = rv1\n",
            "  return rv0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Êü•ÁúãtorchscriptÁîüÊàêÁöÑIR graphË°®Á§∫ÂíåcodeË°®Á§∫\n",
        "import torch\n",
        "@torch.jit.script\n",
        "def foo(len):\n",
        "  # type: (int) -> torch.Tensor\n",
        "  rv = torch.zeros(3, 4)\n",
        "  for i in range(len):\n",
        "    if i < 10:\n",
        "        rv = rv - 1.0\n",
        "    else:\n",
        "        rv = rv + 1.0\n",
        "  return rv\n",
        "\n",
        "print(foo.graph)\n",
        "print(foo.code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read more about [building neural networks in PyTorch](buildmodel_tutorial.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optimizing the Model Parameters\n",
        "To train a model, we need a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "and an [optimizer](https://pytorch.org/docs/stable/optim.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false,
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Optimizing the Model Parameters\n",
        "# To train a model, we need a loss function and an optimizer.\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and\n",
        "backpropagates the prediction error to adjust the model's parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false,
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# a single training loop, the model makes predictions, backpropagates the prediction error, adjust the model‚Äôs parameters.\n",
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset) # size = 60000\n",
        "    model.train() # Sets the module in training mode\n",
        "    for batch, (X, y) in enumerate(dataloader): \n",
        "        X, y = X.to(device), y.to(device) # X.shape = torch.Size([64, 1, 28, 28]), y.shape = torch.Size([64])\n",
        "        \n",
        "        # Compute prediction error\n",
        "        pred = model(X) # pred.shape = torch.Size([64, 10]), 64 batches\n",
        "        # pred[0] = tensor([ 0.1264, -0.0450,  0.0593,  0.0600, -0.0301,  0.0103,  0.0152,  0.1127,\n",
        "        #                              0.0805, -0.1390], device='cuda:0', grad_fn=<SelectBackward0>)\n",
        "        # predÊ¶ÇÁéáÂÄº‰∏≠ÊúâË¥üÊï∞ÊòØÂõ†‰∏∫ÊúÄÂêé‰∏ÄÂ±ÇÊ≤°Êúâ‰ΩøÁî®SigmiodÊøÄÊ¥ªÂáΩÊï∞ÈôêÂà∂ÂÄºÁöÑËåÉÂõ¥Âà∞(0,1)\n",
        "        \n",
        "        loss = loss_fn(pred, y) # type(loss) = 'torch.Tensor', len(loss) = 1\n",
        "        # loss = tensor(2.3079, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
        "\n",
        "        #3 Backpropagation\n",
        "        optimizer.zero_grad() # Sets the gradients of all optimized torch.Tensor s to zero.\n",
        "        loss.backward() # Computes the gradient of current tensor('loss') w.r.t. graph leaves\n",
        "        # every tensor has a grad attribute (None by default and becomes a Tensor \n",
        "        # the first time a call to backward computes gradients for self.  \n",
        "        # and a grad_fn prop in which there stores tensors it is derived from\n",
        "        # ;\n",
        "        # When we do loss.backward() the process of backpropagation starts at the loss \n",
        "        # and goes through all of its parents all the way to model inputs parameter's grad attribute\n",
        "        optimizer.step() # Performs a single optimization step (parameter tensor update).\n",
        "        # use their(parameter's) internally stored grad to update their values.\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}    [{current:>5d}/{size:>5d}]\") # Ê£ÄÈ™ålossÊòØÂê¶ÊúâË¢´‰ºòÂåñ„ÄÅÊòØÂê¶Âú®‰∏ãÈôç"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We also check the model's performance against the test dataset to ensure it is learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false,
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#  check the model‚Äôs performance against the test dataset to ensure it is learning.\n",
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset) \n",
        "    num_batches = len(dataloader)\n",
        "    model.eval() # Sets the module in evaluation mode\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad(): # Context-manager that disabled gradient calculatio\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item() # È¢ÑÊµãÊ≠£Á°ÆÁöÑ‰∏™Êï∞\n",
        "            # torch.argmax() :Returns the indices of the maximum values of a tensor across a dimension\n",
        "            # pred[0] = tensor([[-2.5338, -2.6526, -0.8845, -2.1145, -0.8406,  2.5560, -1.0434,  2.8125, 1.8455,  3.1938]])\n",
        "            # pred.shape = torch.Size([64, 10])\n",
        "            # pred.argmax(1).shape = torch.Size([64])\n",
        "            # (pred.argmax(1) == y).shape = torch.Size([64]) consisted of True or False\n",
        "            # (pred.argmax(1) == y).type(torch.float) consisted of 1. or 2.0\n",
        "            # .sum().item() ËÆ°ÁÆóÈ¢ÑÊµãÊ≠£Á°ÆÁöÑ‰∏™Êï∞\n",
        "    test_loss /= num_batches # n‰∏™batchÁöÑÂπ≥ÂùáÊçüÂ§±\n",
        "    correct /= size # Ê≠£Á°ÆÁéá\n",
        "    # to see the accuracy increase and the loss decrease with every epoch.\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The training process is conducted over several iterations (*epochs*). During each epoch, the model learns\n",
        "parameters to make better predictions. We print the model's accuracy and loss at each epoch; we'd like to see the\n",
        "accuracy increase and the loss decrease with every epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false,
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.287906    [    0/60000]\n",
            "loss: 2.278855    [ 6400/60000]\n",
            "loss: 2.262914    [12800/60000]\n",
            "loss: 2.268467    [19200/60000]\n",
            "loss: 2.235638    [25600/60000]\n",
            "loss: 2.212586    [32000/60000]\n",
            "loss: 2.215851    [38400/60000]\n",
            "loss: 2.175164    [44800/60000]\n",
            "loss: 2.177122    [51200/60000]\n",
            "loss: 2.147410    [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 44.0%, Avg loss: 2.139662 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.144533    [    0/60000]\n",
            "loss: 2.132601    [ 6400/60000]\n",
            "loss: 2.074656    [12800/60000]\n",
            "loss: 2.100196    [19200/60000]\n",
            "loss: 2.038900    [25600/60000]\n",
            "loss: 1.982468    [32000/60000]\n",
            "loss: 2.010017    [38400/60000]\n",
            "loss: 1.924220    [44800/60000]\n",
            "loss: 1.933917    [51200/60000]\n",
            "loss: 1.862314    [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 53.9%, Avg loss: 1.853933 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 2\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")\n",
        "# Accuracy: 64.6%, Avg loss: 1.106282\n",
        "# Accuracy: 70.9%, Avg loss: 0.783265 (epoch=20)\n",
        "# with sigmoid Accuracy: 45.5%, Avg loss: 2.245753\n",
        "# with softmax Accuracy: 10.4%, Avg loss: 2.294693 "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read more about [Training your model](optimization_tutorial.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Saving Models\n",
        "A common way to save a model is to serialize the internal state dictionary (containing the model parameters).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": false,
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved PyTorch Model State to model.pth\n"
          ]
        }
      ],
      "source": [
        "# Saving Models\n",
        "torch.save(model.state_dict(), \"model.pth\") #  serialize the internal state dictionary (containing the model parameters)\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading Models\n",
        "\n",
        "The process for loading a model includes re-creating the model structure and loading\n",
        "the state dictionary into it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false,
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Loading Models: re-creating the model structure and loading the state dictionary into it\n",
        "model = NeuralNetwork() # re-creating the model structure\n",
        "model.load_state_dict(torch.load(\"model.pth\")) # loading the state dictionary, torch.load() Loads an object saved with torch.save from a file. \n",
        "# type(torch.load(\"model.pth\")) = collections.OrderedDict\n",
        "# model.pth, model.state_dict(), model.parameters()\n",
        "# optimizer.param_groups[0]['params']‰∏≠ÂåÖÂê´ÁöÑÈÉΩÊòØÊâÄÊúâÁΩëÁªúÂ±Ç‰∏≠ÁöÑÊùÉÈáçÁ≥ªÊï∞Áü©ÈòµÂíåÂÅèÂ∑ÆÁü©ÈòµÁöÑÂÄº"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This model can now be used to make predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false,
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted: \"Ankle boot\", Actual: \"Ankle boot\"\n"
          ]
        }
      ],
      "source": [
        "classes = [\n",
        "    \"T-shirt/top\",\n",
        "    \"Trouser\",\n",
        "    \"Pullover\",\n",
        "    \"Dress\",\n",
        "    \"Coat\",\n",
        "    \"Sandal\",\n",
        "    \"Shirt\",\n",
        "    \"Sneaker\",\n",
        "    \"Bag\",\n",
        "    \"Ankle boot\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "\n",
        "\n",
        "x, y = test_data[0][0], test_data[0][1]\n",
        "with torch.no_grad():\n",
        "    pred = model(x)\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y] # pred[0].argmax(0) = pred.argmax(1)[0]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Read more about [Saving & Loading your model](saveloadrun_tutorial.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ‰∫å„ÄÅÂÖ•Èó®ÊïôÁ®ãpytorch basic\n",
        "## tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "tensor([[1, 2],\n",
            "        [3, 4]])\n",
            "tensor([[1, 1],\n",
            "        [1, 1]])\n",
            "tensor([[0.4610, 0.6206],\n",
            "        [0.1566, 0.3326]])\n",
            "tensor([[0.1460, 0.5895, 0.6048],\n",
            "        [0.0745, 0.1240, 0.3872]])\n",
            "tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "#  In PyTorch, we use tensors to encode the inputs and outputs of a model, as well as the model‚Äôs parameters.\n",
        "import torch\n",
        "# Tensors are similar to NumPy‚Äôs ndarrays, except that tensors can run on GPUs,  optimized for automatic differentiation - Autograd \n",
        "# In fact, tensors and NumPy arrays can often share the same underlying memory(Tensors on the CPU), eliminating the need to copy data\n",
        "import numpy as np\n",
        "\n",
        "# 1. initializing a tensor\n",
        "# directly from data\n",
        "data = [[1,2], [3,4]]\n",
        "x_data = torch.tensor(data)\n",
        "print(x_data)\n",
        "\n",
        "# from a numpy array\n",
        "np_array = np.array(data)\n",
        "print(torch.from_numpy(np_array))\n",
        "\n",
        "# from another tensor\n",
        "x_ones = torch.ones_like(x_data)\n",
        "print(x_ones)\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
        "print(x_rand)\n",
        "\n",
        "# with random or constant values\n",
        "shape = (2,3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "one_tensor = torch.ones(shape)\n",
        "zero_tensor = torch.zeros(shape)\n",
        "print(rand_tensor)\n",
        "print(one_tensor)\n",
        "print(zero_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is store on: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# 2. attributes of a tensor \n",
        "# tensor has other attribute such like: grad, grad_fn ...\n",
        "tensor = torch.rand(3, 4 ,device='cuda')\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is store on: {tensor.device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First row: tensor([1., 1., 1., 1.])\n",
            "First column: tensor([1., 1., 1., 1.])\n",
            "Last column: tensor([1., 1., 1., 1.])\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n",
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n",
            "matrix multiplication\n",
            " tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]]) \n",
            " tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]]) \n",
            " tensor([[3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.],\n",
            "        [3., 3., 3., 3.]])\n"
          ]
        }
      ],
      "source": [
        "# 3. operations on tensor\n",
        "# move dtensor to gpu\n",
        "if torch.cuda.is_available():\n",
        "    tensor = tensor.to('cuda:0')\n",
        "\n",
        "# numpy-like indexing and slicing\n",
        "tensor = torch.ones(4,4)\n",
        "print(f'First row: {tensor[0]}')\n",
        "print(f'First column: {tensor[:,0]}')\n",
        "print(f'Last column: {tensor[..., -1]}')\n",
        "tensor[:,1] = 0\n",
        "print(tensor)\n",
        "\n",
        "# joining tensors\n",
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)\n",
        "\n",
        "# arithmetic operations\n",
        "# matrix multiplication\n",
        "y1 = tensor @ tensor.T\n",
        "y2 = tensor.matmul(tensor.T)\n",
        "\n",
        "y3 = torch.rand_like(tensor)\n",
        "torch.matmul(tensor, tensor.T, out=y3)\n",
        "\n",
        "print(tensor)\n",
        "print('matrix multiplication\\n', y1,'\\n', y2, '\\n', y3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n",
            "element-wise product\n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n",
            "12.0 <class 'float'>\n",
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n",
            "\n",
            "tensor([[6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.],\n",
            "        [6., 5., 6., 6.]])\n"
          ]
        }
      ],
      "source": [
        "# element-wise product\n",
        "z1 = tensor * tensor\n",
        "z2 = tensor.mul(tensor)\n",
        "\n",
        "z3 = torch.rand_like(tensor)\n",
        "torch.mul(tensor, tensor, out=z3)\n",
        "\n",
        "print(tensor)\n",
        "print('element-wise product\\n', z1,'\\n', z2, '\\n', z3)\n",
        "\n",
        "# single-element tensors\n",
        "agg = tensor.sum()\n",
        "agg_item = agg.item()\n",
        "print(agg_item, type(agg_item))\n",
        "\n",
        "# inplace operations\n",
        "print(f'{tensor}\\n')\n",
        "tensor.add_(5)\n",
        "print(tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "t: tensor([1., 1., 1., 1., 1.])\n",
            "n: [1. 1. 1. 1. 1.]\n",
            "t: tensor([2., 2., 2., 2., 2.])\n",
            "n: [2. 2. 2. 2. 2.]\n",
            "t: tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n",
            "n: [2. 2. 2. 2. 2.]\n"
          ]
        }
      ],
      "source": [
        "# 3.bridge with numpy (Tensors on the CPU)\n",
        "# tensor to numpy array\n",
        "t = torch.ones(5)\n",
        "print(f't: {t}')\n",
        "# t = t.to(\"cuda\") # Ê≥®ÊÑèÂ¶ÇÊûúÊ≤°ÊúâÂ∑¶‰æßÁöÑt=, Áõ¥Êé•ÊâßË°åt.to(device)Ê≤°Êúâ‰ºöÂØºËá¥gpu‰∏≠ÁöÑÊï∞ÊçÆÊ≤°ÊúâË¢´ÂèòÈáèÂºïÁî®\n",
        "# print(t.device)\n",
        "n = t.numpy() # TypeError: can't convert cuda:0 device type tensor to numpy. Use x = Tensor.cpu() to copy the tensor to host memory first.\n",
        "# print(t.device)\n",
        "print(f'n: {n}')\n",
        "\n",
        "# A change in the tensor reflects in the NumPy array.\n",
        "t.add_(1)\n",
        "print(f't: {t}')\n",
        "print(f'n: {n}')\n",
        "\n",
        "# numpy array to tensor\n",
        "n = np.ones(5)\n",
        "t= torch.from_numpy(n)\n",
        "\n",
        "np.add(n, 1, out=n)\n",
        "print(f\"t: {t}\")\n",
        "print(f\"n: {n}\") # when print, ndarray remove all commas(2023-2-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## datasets & dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# dataset code to be decoupled from our model training code for better readability and modularity.\n",
        "# PyTorch provides two data primitives: torch.utils.data.DataLoader and .Dataset to preload data before traini\n",
        "# 1. loading a dataset\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 576x576 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\"\n",
        "}\n",
        "figure = plt.figure(figsize=(8,8))\n",
        "cols, rows = 3, 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 576x576 with 9 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 2. iterating and visualizing the dataset\n",
        "# plt: figure -> subplot -> plt -> axis,title,imshow\n",
        "labels_map = {\n",
        "    0: \"T-Shirt\",\n",
        "    1: \"Trouser\",\n",
        "    2: \"Pullover\",\n",
        "    3: \"Dress\",\n",
        "    4: \"Coat\",\n",
        "    5: \"Sandal\",\n",
        "    6: \"Shirt\",\n",
        "    7: \"Sneaker\",\n",
        "    8: \"Bag\",\n",
        "    9: \"Ankle Boot\",\n",
        "}\n",
        "figure = plt.figure(figsize=(8,8)) # Create a new figure, or activate an existing figure.\n",
        "cols, rows = 3, 3\n",
        "for i in range(1, cols * rows + 1): # subplot index from 1 not 0\n",
        "    sample_idx = torch.randint(len(training_data), size=(1,)).item() # tensorÁîüÊàêÈöèÊú∫Êï¥Êï∞ÔºåÁî®‰∫éÊäΩÊ†∑\n",
        "    img, label = training_data[sample_idx] # type: ignore # tuple unpack\n",
        "    figure.add_subplot(rows, cols, i) # Create a figure containing a single axes in position (rows, cols, i)\n",
        "    plt.title(labels_map[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "plt.show()\n",
        "# ÊàëÂëÜÂùêÂú®Ê§ÖÂ≠ê‰∏äÔºåÂøÉÊÉ≥Ôºö‰∏çËÉΩÂêß„ÄÇÊòØÁöÑÔºå‰∏çËÉΩ‰∏çÂêÉÈ•≠ÔºåË∑ØËøáÁöÑÈ∏üÂÑøÂñÑÊÑèÊèêÈÜíÊàë„ÄÇ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# 3. creating a custom dataset for your files\n",
        "# A custom Dataset class must implement three functions: __init__, __len__, and __getitem__\n",
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.io import read_image\n",
        "\n",
        "class CustomImageDataset(Dataset):\n",
        "    def __inin__(self, annotations_file, img__dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img__dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# 4. preparing your data for training with dataloaders\n",
        "# The Dataset retrieves our dataset‚Äôs features and labels one(Dataset) sample at a time. While training a model, \n",
        "# we typically want to pass samples in ‚Äúminibatches‚Äù(DataLoader), reshuffle the data at every epoch\n",
        "from torch.utils.data import DataLoader\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature batch shape: torch.Size([64, 1, 28, 28])\n",
            "Labels batch shape: torch.Size([64])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label: 3\n"
          ]
        }
      ],
      "source": [
        "# 5. iterate through the dataloader\n",
        "# Display image and label\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "img = train_features[0].squeeze()\n",
        "label = train_labels[0]\n",
        "plt.imshow(img, cmap='gray')\n",
        "plt.show()\n",
        "print(f\"Label: {label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# all torchvison datasets have two parameters: transform and traget_transform\n",
        "# use transforms to perform some manipulation of the data and make it suitable for training\n",
        "import torch\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "# Lambda transforms apply any user-defined lambda function\n",
        "\n",
        "ds = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        "    target_transform=Lambda(lambda y: torch.zeros(10, dtype=torch.float).scatter_(0, torch.tensor(y), value=1)) # int to one-hot\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0., 0.],\n",
            "        [0., 1., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0.]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<__main__.onehotScatter at 0x7f6d70bca2e0>"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# torch.Tensor.scatter_()ÊääÊ†áÈáèÁöÑÊ†áÁ≠æËΩ¨Êç¢‰∏∫one-hotÁºñÁ†Å\n",
        "class onehotScatter():\n",
        "    def __init__(self, batch_size=4, class_num=5, labels=torch.tensor([4,0,1,2])):\n",
        "        self.batch_size = batch_size\n",
        "        self.class_num = class_num\n",
        "        self.labels = torch.unsqueeze(labels, 1) # Êâ©Â±ïÊàê‰∫åÁª¥\n",
        "        self.one_hot = torch.zeros(batch_size, class_num)\n",
        "        _dim = 1; _index_tensor = self.labels; _src = 1\n",
        "        self.one_hot.scatter_(_dim, _index_tensor, _src)\n",
        "        print(self.one_hot)\n",
        "\n",
        "onehotScatter()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## build model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# build the neural network\n",
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "NeuralNetwork(\n",
            "  (faltten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "torch.Size([1, 784])\n",
            "tensor([[-0.0848, -0.0506, -0.0014, -0.0202, -0.0349,  0.0420, -0.0501, -0.0389,\n",
            "          0.0633, -0.0192]], device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor([[0.0936, 0.0968, 0.1017, 0.0998, 0.0984, 0.1062, 0.0969, 0.0980, 0.1085,\n",
            "         0.0999]], device='cuda:0', grad_fn=<SoftmaxBackward0>)\n",
            "Predicted class: tensor([8], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "# 1. get device for training\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# 2. define the class, nn.Module is directly inherit from class 'object', not isinstance of tensor\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.faltten = nn.Flatten() # start_dim: int = 1 end_dim=-1\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.faltten(x)\n",
        "        print(x.shape)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "    \n",
        "# create an instance of NeuralNetwork and move to the device\n",
        "model = NeuralNetwork().to(device) # to() defined in nn.Module\n",
        "print(model) # __str__not exists, and execute nn.Module.__repr__()\n",
        "\n",
        "# to use the model, pass it the inpyt data, auto execute forward() returna raw predicted values\n",
        "X = torch.rand(1, 28, 28, device=device)\n",
        "logits = model(X) # ÈªòËÆ§model.training=True\n",
        "print(logits)\n",
        "pred_probab = nn.Softmax(dim=1)(logits) # range [0,1] and sum to 1\n",
        "print(pred_probab)\n",
        "y_pred = pred_probab.argmax(1) # return index\n",
        "print(f\"Predicted class: {y_pred}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 28, 28])\n",
            "torch.Size([3, 784])\n",
            "torch.Size([3, 20])\n",
            "Before ReLU: tensor([[-1.4547e-01, -2.1345e-01,  2.8351e-01,  8.4186e-04, -4.1900e-01,\n",
            "          2.9845e-01, -5.6992e-01,  2.9261e-01,  3.3209e-03, -1.4056e-01,\n",
            "          3.6584e-01,  2.0745e-01,  3.9022e-02,  4.3132e-01, -6.3633e-02,\n",
            "          7.8021e-01, -1.3780e-01,  1.1658e-01, -9.7171e-02, -3.3598e-01],\n",
            "        [ 9.8997e-02, -1.7489e-01,  1.2345e-01,  2.6564e-01, -4.4254e-02,\n",
            "          2.3046e-01, -1.4765e-01,  3.6650e-01, -2.6805e-02, -2.2589e-01,\n",
            "         -8.8113e-02,  5.5007e-01,  3.5191e-01,  4.1690e-01, -1.5945e-01,\n",
            "          8.8372e-01, -1.3807e-01, -3.4014e-02, -2.1347e-01, -4.9382e-01],\n",
            "        [-2.2000e-01, -1.3019e-01,  1.5604e-01,  3.5273e-01, -3.9375e-01,\n",
            "          6.6977e-02, -6.1013e-01, -3.6730e-02, -2.1297e-01,  1.7760e-01,\n",
            "          2.1652e-01, -3.5736e-02,  2.6910e-01,  3.8336e-01, -1.7345e-01,\n",
            "          5.0293e-01,  2.1078e-02, -9.9262e-02, -2.6981e-01, -2.5420e-01]],\n",
            "       grad_fn=<AddmmBackward0>)\n",
            "\n",
            "\n",
            "After ReLU: tensor([[0.0000e+00, 0.0000e+00, 2.8351e-01, 8.4186e-04, 0.0000e+00, 2.9845e-01,\n",
            "         0.0000e+00, 2.9261e-01, 3.3209e-03, 0.0000e+00, 3.6584e-01, 2.0745e-01,\n",
            "         3.9022e-02, 4.3132e-01, 0.0000e+00, 7.8021e-01, 0.0000e+00, 1.1658e-01,\n",
            "         0.0000e+00, 0.0000e+00],\n",
            "        [9.8997e-02, 0.0000e+00, 1.2345e-01, 2.6564e-01, 0.0000e+00, 2.3046e-01,\n",
            "         0.0000e+00, 3.6650e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 5.5007e-01,\n",
            "         3.5191e-01, 4.1690e-01, 0.0000e+00, 8.8372e-01, 0.0000e+00, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00],\n",
            "        [0.0000e+00, 0.0000e+00, 1.5604e-01, 3.5273e-01, 0.0000e+00, 6.6977e-02,\n",
            "         0.0000e+00, 0.0000e+00, 0.0000e+00, 1.7760e-01, 2.1652e-01, 0.0000e+00,\n",
            "         2.6910e-01, 3.8336e-01, 0.0000e+00, 5.0293e-01, 2.1078e-02, 0.0000e+00,\n",
            "         0.0000e+00, 0.0000e+00]], grad_fn=<ReluBackward0>)\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 3. model layers\n",
        "\n",
        "# break down the layers in the model, a sample minibatch of 3 images of size 28x28\n",
        "# and see what happens to it as we pass it through the network\n",
        "input_images = torch.rand(3, 28, 28)\n",
        "print(input_images.size())\n",
        "\n",
        "# 3.1 nn.flatten\n",
        "flatten = nn.Flatten() # convert each 2D 28x28 image into a contiguous array of 784 pixel values, the minibatch dimension (at dim=0) is maintained\n",
        "flat_image = flatten(input_images)\n",
        "print(flat_image.size())\n",
        "\n",
        "# 3.2 nn.Linear\n",
        "layer1 = nn.Linear(in_features=28*28, out_features=20) # linear layer is a module that applies a linear transformation on the input\n",
        "hidden1 = layer1(flat_image)\n",
        "print(hidden1.size())\n",
        "\n",
        "# 3.3 nn.ReLU\n",
        "print(f\"Before ReLU: {hidden1}\\n\\n\")\n",
        "hidden1 = nn.ReLU()(hidden1) # introduce non-linearity in your model, use nn.ReLU between our linear layers,\n",
        "print(f\"After ReLU: {hidden1}\\n\\n\")\n",
        "\n",
        "# 3.4 nn.Sequential\n",
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1,\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20, 10),\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)\n",
        "\n",
        "# 3.5 nn.Softmax\n",
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model struct: NeuralNetwork(\n",
            "  (faltten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[-0.0323, -0.0004,  0.0189,  ...,  0.0318, -0.0325, -0.0196],\n",
            "        [ 0.0156,  0.0330, -0.0120,  ..., -0.0228,  0.0067, -0.0039]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0211,  0.0005], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0012,  0.0402, -0.0061,  ...,  0.0432, -0.0376,  0.0102],\n",
            "        [ 0.0329,  0.0416, -0.0229,  ..., -0.0358, -0.0441,  0.0145]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0195, -0.0352], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[ 0.0257,  0.0263,  0.0177,  ..., -0.0261, -0.0100, -0.0036],\n",
            "        [-0.0246, -0.0396,  0.0274,  ...,  0.0088,  0.0116, -0.0003]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([-0.0274, -0.0412], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 4. model parameters\n",
        "print(f\"Model struct: {model}\\n\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## autograd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([1., 1., 1., 1., 1.]),\n",
              " tensor([0., 0., 0.]),\n",
              " tensor([-2.9735,  1.3257,  1.1707], grad_fn=<AddBackward0>))"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# In back propagation algorithm, parameters (model weights) are adjusted according to \n",
        "# the gradient of the loss function with respect to the given parameter.\n",
        "# automatic differentitatiom with torch.autograd for any computational graph\n",
        "\n",
        "# Consider the simplest one-layer neural network, with input x, parameters w and b, and some loss function\n",
        "import torch\n",
        "\n",
        "x = torch.ones(5) # input tensor\n",
        "y = torch.zeros(3) # expected output\n",
        "w = torch.randn(5, 3, requires_grad=True)\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "z = torch.matmul(x, w) + b\n",
        "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
        "x,y,z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.],\n",
              "         [1., 1., 1.]]),\n",
              " tensor([1., 1., 1.]),\n",
              " <AddBackward0 at 0x7ff987593460>)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "z.backward(torch.ones_like(z)) # ÊµãËØï‰ªé‰∏≠Èó¥ËäÇÁÇπÊâßË°åbackward()Ëá™Âä®Ê¢ØÂ∫¶\n",
        "w.grad, b.grad, z.grad_fn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([0., 0., 0.]), tensor([0., 0., 0.]))"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ê∏ÖÁ©∫Âè∂Â≠êËäÇÁÇπÊ¢ØÂ∫¶\n",
        "b.grad.zero_(), b.grad.zero_()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient function for z = <AddBackward0 object at 0x7ff987593460>\n",
            "Gradient function for loss = <BinaryCrossEntropyWithLogitsBackward0 object at 0x7ff9873a6b50>\n"
          ]
        }
      ],
      "source": [
        "# A function that we apply to tensors to construct computational graph is in fact an object of class Function. \n",
        "# This object knows how to compute the function in the forward direction, \n",
        "# and also how to compute its derivative during the backward propagation step\n",
        "\n",
        "# backward propagation function is stored in grad_fn property of a tensor\n",
        "print(f\"Gradient function for z = {z.grad_fn}\")\n",
        "print(f\"Gradient function for loss = {loss.grad_fn}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.0162, 0.2634, 0.2544],\n",
            "        [0.0162, 0.2634, 0.2544],\n",
            "        [0.0162, 0.2634, 0.2544],\n",
            "        [0.0162, 0.2634, 0.2544],\n",
            "        [0.0162, 0.2634, 0.2544]])\n",
            "tensor([0.0162, 0.2634, 0.2544])\n"
          ]
        }
      ],
      "source": [
        "# compute the derivatives of our loss function with respect to parameters\n",
        "# to compute those derivatives, we call loss.backward(), and then retrieve the values from w.grad and b.grad\n",
        "loss.backward() # We can only perform gradient calculations using backward once on a given graph\n",
        "# If we need to do several backward calls on the same graph, pass retain_graph=True to the backward call\n",
        "print(w.grad)\n",
        "print(b.grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "# diable gradient tracking:1.mark as frozen parameters -> z.detach()\n",
        "# 2. speed up computations when only doing forward pass -> with torch.no_grad():\n",
        "z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)\n",
        "# ÊâÄÊúâ‰æùËµñ‰∫éÂè∂Â≠êËäÇÁÇπÂº†ÈáèÁöÑÂº†ÈáèÔºåÂÖ∂ requires_grad Â±ûÊÄßÂøÖÂÆöÊòØ True ÁöÑÔºå‰ΩÜÂÖ∂Ê¢ØÂ∫¶ÂÄºÂè™Âú®ËÆ°ÁÆóËøáÁ®ã‰∏≠Ë¢´Áî®Âà∞Ôºå\n",
        "# ‰∏ç‰ºöÊúÄÁªàÂ≠òÂÇ®Âà∞ grad Â±ûÊÄß‰∏≠„ÄÇÂ¶ÇÊûúÈúÄË¶Å‰øùÁïô‰∏≠Èó¥ËÆ°ÁÆóÁªìÊûúÁöÑÊ¢ØÂ∫¶Âà∞ grad Â±ûÊÄß‰∏≠ÔºåÂèØ‰ª•‰ΩøÁî® retain_grad ÊñπÊ≥ï„ÄÇ \n",
        "# Â¶ÇÊûú‰ªÖ‰ªÖÊòØ‰∏∫‰∫ÜË∞ÉËØï‰ª£Á†ÅÊü•ÁúãÊ¢ØÂ∫¶ÂÄºÔºåÂèØ‰ª•Âà©Áî® register_hook ÊâìÂç∞Êó•Âøó\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "False\n"
          ]
        }
      ],
      "source": [
        "z = torch.matmul(x, w)+b\n",
        "z_det = z.detach() # mark as frozen parameters stop gradient computing tracking\n",
        "print(z_det.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First call\n",
            "tensor([[4., 2., 2., 2., 2.],\n",
            "        [2., 4., 2., 2., 2.],\n",
            "        [2., 2., 4., 2., 2.],\n",
            "        [2., 2., 2., 4., 2.]])\n",
            "\n",
            "Second call\n",
            "tensor([[8., 4., 4., 4., 4.],\n",
            "        [4., 8., 4., 4., 4.],\n",
            "        [4., 4., 8., 4., 4.],\n",
            "        [4., 4., 4., 8., 4.]])\n",
            "\n",
            "Third call\n",
            "tensor([[4., 2., 2., 2., 2.],\n",
            "        [2., 4., 2., 2., 2.],\n",
            "        [2., 2., 4., 2., 2.],\n",
            "        [2., 2., 2., 4., 2.]])\n"
          ]
        }
      ],
      "source": [
        "# More on Computational Graphs, DAGs are dynamic in PyTorch.\n",
        "# After each .backward() call, autograd starts populating a new graph\n",
        "# Tensor Gradients and Jacobian Products\n",
        "inp = torch.eye(4, 5, requires_grad=True)\n",
        "out = (inp+1).pow(2).t() # .t() transpose()‰∫åÁª¥ÁÆÄÂåñÁâàÔºåËΩ¨ÁΩÆ‰∫åÁª¥Âº†Èáè\n",
        "out.backward(torch.ones_like(out), retain_graph=True) # compute Jacobian Product, Ê†áÈáèÂíåÂº†ÈáèÁöÑËá™Âä®Ê¢ØÂ∫¶backward\n",
        "# retain_graph=True ‰øùÊåÅËÆ°ÁÆóÂõæ\n",
        "print(f\"First call\\n{inp.grad}\")\n",
        "\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"\\nSecond call\\n{inp.grad}\")  # when doing backward propagation, PyTorch accumulates the gradients\n",
        "\n",
        "inp.grad.zero_() # inpËäÇÁÇπÈáçÁΩÆÊ¢ØÂ∫¶ÂÄº‰∏∫0\n",
        "out.backward(torch.ones_like(out), retain_graph=True)\n",
        "print(f\"\\nThird call\\n{inp.grad}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Prerequiste Code for data and model construction\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "training_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NeuralNetwork, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "model = NeuralNetwork()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Hyperparameters\n",
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# initialize the optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Optimizer: optimization loop(epoch) -> training loop/validation or test loop -> batchsize to update parameters\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)   \n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropage\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}    [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches    # ËÆ°ÁÆóÊØèbatchÁöÑÂπ≥ÂùáÊçüÂ§±Avg loss\n",
        "    correct /= size             # ËÆ°ÁÆó%Ê≠£Á°ÆÁéá\n",
        "    print(f\"Testor Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "--------------------------------------\n",
            "loss: 2.301069    [    0/60000]\n",
            "loss: 2.286863    [ 6400/60000]\n",
            "loss: 2.269864    [12800/60000]\n",
            "loss: 2.263703    [19200/60000]\n",
            "loss: 2.238540    [25600/60000]\n",
            "loss: 2.212242    [32000/60000]\n",
            "loss: 2.223095    [38400/60000]\n",
            "loss: 2.192458    [44800/60000]\n",
            "loss: 2.196774    [51200/60000]\n",
            "loss: 2.145911    [57600/60000]\n",
            "Testor Error: \n",
            " Accuracy: 51.7, Avg loss: 2.145273 \n",
            "\n",
            "Epoch 2\n",
            "--------------------------------------\n",
            "loss: 2.162574    [    0/60000]\n",
            "loss: 2.148090    [ 6400/60000]\n",
            "loss: 2.096596    [12800/60000]\n",
            "loss: 2.106218    [19200/60000]\n",
            "loss: 2.054029    [25600/60000]\n",
            "loss: 1.997180    [32000/60000]\n",
            "loss: 2.028912    [38400/60000]\n",
            "loss: 1.956131    [44800/60000]\n",
            "loss: 1.970566    [51200/60000]\n",
            "loss: 1.873679    [57600/60000]\n",
            "Testor Error: \n",
            " Accuracy: 54.6, Avg loss: 1.878449 \n",
            "\n",
            "Epoch 3\n",
            "--------------------------------------\n",
            "loss: 1.923189    [    0/60000]\n",
            "loss: 1.885918    [ 6400/60000]\n",
            "loss: 1.778721    [12800/60000]\n",
            "loss: 1.804792    [19200/60000]\n",
            "loss: 1.698741    [25600/60000]\n",
            "loss: 1.652026    [32000/60000]\n",
            "loss: 1.675935    [38400/60000]\n",
            "loss: 1.582891    [44800/60000]\n",
            "loss: 1.606894    [51200/60000]\n",
            "loss: 1.486196    [57600/60000]\n",
            "Testor Error: \n",
            " Accuracy: 59.5, Avg loss: 1.509655 \n",
            "\n",
            "Epoch 4\n",
            "--------------------------------------\n",
            "loss: 1.582333    [    0/60000]\n",
            "loss: 1.544717    [ 6400/60000]\n",
            "loss: 1.402078    [12800/60000]\n",
            "loss: 1.460636    [19200/60000]\n",
            "loss: 1.343524    [25600/60000]\n",
            "loss: 1.340030    [32000/60000]\n",
            "loss: 1.357782    [38400/60000]\n",
            "loss: 1.284630    [44800/60000]\n",
            "loss: 1.310531    [51200/60000]\n",
            "loss: 1.212157    [57600/60000]\n",
            "Testor Error: \n",
            " Accuracy: 63.2, Avg loss: 1.239347 \n",
            "\n",
            "Epoch 5\n",
            "--------------------------------------\n",
            "loss: 1.314731    [    0/60000]\n",
            "loss: 1.301104    [ 6400/60000]\n",
            "loss: 1.138467    [12800/60000]\n",
            "loss: 1.237033    [19200/60000]\n",
            "loss: 1.111356    [25600/60000]\n",
            "loss: 1.138476    [32000/60000]\n",
            "loss: 1.166731    [38400/60000]\n",
            "loss: 1.104499    [44800/60000]\n",
            "loss: 1.131373    [51200/60000]\n",
            "loss: 1.056238    [57600/60000]\n",
            "Testor Error: \n",
            " Accuracy: 64.9, Avg loss: 1.075788 \n",
            "\n",
            "Epoch 6\n",
            "--------------------------------------\n",
            "loss: 1.142474    [    0/60000]\n",
            "loss: 1.151956    [ 6400/60000]\n",
            "loss: 0.969813    [12800/60000]\n",
            "loss: 1.100241    [19200/60000]\n",
            "loss: 0.971795    [25600/60000]\n",
            "loss: 1.008955    [32000/60000]\n",
            "loss: 1.053535    [38400/60000]\n",
            "loss: 0.995078    [44800/60000]\n",
            "loss: 1.019035    [51200/60000]\n",
            "loss: 0.960895    [57600/60000]\n",
            "Testor Error: \n",
            " Accuracy: 66.0, Avg loss: 0.972783 \n",
            "\n",
            "Epoch 7\n",
            "--------------------------------------\n",
            "loss: 1.027018    [    0/60000]\n",
            "loss: 1.057593    [ 6400/60000]\n",
            "loss: 0.857486    [12800/60000]\n",
            "loss: 1.010034    [19200/60000]\n",
            "loss: 0.885458    [25600/60000]\n",
            "loss: 0.920451    [32000/60000]\n",
            "loss: 0.981357    [38400/60000]\n",
            "loss: 0.925824    [44800/60000]\n",
            "loss: 0.942459    [51200/60000]\n",
            "loss: 0.896866    [57600/60000]\n",
            "Testor Error: \n",
            " Accuracy: 67.3, Avg loss: 0.903110 \n",
            "\n",
            "Epoch 8\n",
            "--------------------------------------\n",
            "loss: 0.943129    [    0/60000]\n",
            "loss: 0.992107    [ 6400/60000]\n",
            "loss: 0.778119    [12800/60000]\n",
            "loss: 0.945797    [19200/60000]\n",
            "loss: 0.828160    [25600/60000]\n",
            "loss: 0.856671    [32000/60000]\n",
            "loss: 0.930688    [38400/60000]\n",
            "loss: 0.879960    [44800/60000]\n",
            "loss: 0.887271    [51200/60000]\n",
            "loss: 0.850303    [57600/60000]\n",
            "Testor Error: \n",
            " Accuracy: 68.5, Avg loss: 0.852758 \n",
            "\n",
            "Epoch 9\n",
            "--------------------------------------\n",
            "loss: 0.878406    [    0/60000]\n",
            "loss: 0.942516    [ 6400/60000]\n",
            "loss: 0.719107    [12800/60000]\n",
            "loss: 0.897403    [19200/60000]\n",
            "loss: 0.787340    [25600/60000]\n",
            "loss: 0.808684    [32000/60000]\n",
            "loss: 0.891989    [38400/60000]\n",
            "loss: 0.847959    [44800/60000]\n",
            "loss: 0.845645    [51200/60000]\n",
            "loss: 0.814124    [57600/60000]\n",
            "Testor Error: \n",
            " Accuracy: 69.5, Avg loss: 0.814231 \n",
            "\n",
            "Epoch 10\n",
            "--------------------------------------\n",
            "loss: 0.826253    [    0/60000]\n",
            "loss: 0.902205    [ 6400/60000]\n",
            "loss: 0.673186    [12800/60000]\n",
            "loss: 0.859635    [19200/60000]\n",
            "loss: 0.756401    [25600/60000]\n",
            "loss: 0.771387    [32000/60000]\n",
            "loss: 0.860212    [38400/60000]\n",
            "loss: 0.824113    [44800/60000]\n",
            "loss: 0.812824    [51200/60000]\n",
            "loss: 0.784453    [57600/60000]\n",
            "Testor Error: \n",
            " Accuracy: 70.9, Avg loss: 0.783265 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# initialize the loss function and optimizer, and pass it to train_loop and test_loop\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n--------------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")\n",
        "# Accuracy: 70.9%, Avg loss: 0.783265 (epoch=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# warmstarting a model using parameters of a different model.\n",
        "# set the strict argument to False in the load_state_dict() function to ignore non-matching keys\n",
        "\n",
        "# Import necessary libraries for loading our data\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Define and intialize the neural network A and B\n",
        "class NetA(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetA, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "netA = NetA()\n",
        "\n",
        "class NetB(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetB, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        # self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        # x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "netB = NetB()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "#Âú®tensorboard‰∏≠Êü•ÁúãÊ®°Âûã\n",
        "writer = SummaryWriter('./data/tensorboard')\n",
        "writer.add_graph(model, input_to_model = torch.randn(1,3,32,32)\n",
        "writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.3643,  1.4855,  0.5947, -0.9331],\n",
              "        [-1.0596, -0.3479, -1.4613, -0.3209]])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.randn((2,4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Save model A\n",
        "# Specify a path to save to\n",
        "PATH = \"model_NetA.pt\"\n",
        "\n",
        "torch.save(netA.state_dict(), PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "_IncompatibleKeys(missing_keys=[], unexpected_keys=['fc3.weight', 'fc3.bias'])"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#  Load into model B\n",
        "netB.load_state_dict(torch.load(PATH), strict=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# visualize netA in netron\n",
        "netaJit = torch.jit.script(netA)     # need transform to torchscript\n",
        "netaJit.save(\"netaJit.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class NetronTest(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(NetronTest, self).__init__()        \n",
        "\n",
        "    def forward(self, x): \n",
        "        x = x * 2 \n",
        "        x.add_(0) \n",
        "        x = x.view(-1) \n",
        "        if x[0] > 1: \n",
        "            return x[0] \n",
        "        else: \n",
        "            return x[-1]\n",
        "\n",
        "netronTest = NetronTest()   # netronTest.state_dict() is empty, cannot recognized by netron directly\n",
        "jit_script_model = torch.jit.script(netronTest)     # need transform to torchscript\n",
        "jit_script_model.save(\"netronTestJit.pth\")\n",
        "# then upload file \"netronTestJit.pth\" to netron website: https://netron.app/, for visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## save and load the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# persist model state with saving\n",
        "import torch\n",
        "import torchvision.models as models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/zhoujiaming/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100.0%\n"
          ]
        }
      ],
      "source": [
        "model = models.vgg16(pretrained=True)\n",
        "# learned parameters in an internal state dictionary, called state_dict\n",
        "torch.save(model.state_dict(), \"model_weights.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VGG(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (18): ReLU(inplace=True)\n",
              "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (25): ReLU(inplace=True)\n",
              "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (27): ReLU(inplace=True)\n",
              "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (29): ReLU(inplace=True)\n",
              "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Dropout(p=0.5, inplace=False)\n",
              "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
              "    (4): ReLU(inplace=True)\n",
              "    (5): Dropout(p=0.5, inplace=False)\n",
              "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# To load model weights, you need to create an instance of the same model first, and then load the parameters \n",
        "model = models.vgg16() # we do not specify pretrained=True, i.e. do not load default weights\n",
        "model.load_state_dict(torch.load('model_weights.pth'))\n",
        "model.eval() # all model.eval() method before inferencing to set the dropout and batch normalization layers to evaluation mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Saving and Loading Models with Shapes\n",
        "# want to save the structure of this class together with the model, pass model (and not model.state_dict()) to the saving function\n",
        "torch.save(model, \"model_save.pth\")\n",
        "\n",
        "# load the model\n",
        "model = torch.load(\"model_save.pth\")\n",
        "# This approach uses Python pickle module when serializing the model, \n",
        "# thus it relies on the actual class definition to be available when loading the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## end at 2023-2-5 22:29:42"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:cdconv]",
      "language": "python",
      "name": "conda-env-cdconv-py"
    },
    "vscode": {
      "interpreter": {
        "hash": "fcfb5a621c0edbf786756912908d3d80a2726106c02d9512755c0aff2e95f196"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
